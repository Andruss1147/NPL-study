{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.1.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.6 (default, Oct 26 2016 20:30:19)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--packages com.databricks:spark-csv_2.10:1.2.0 pyspark-shell'\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, spark_home + \"/python\")\n",
    "execfile(os.path.join(spark_home, 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = spark.read.csv(\"/labs/lab10data/lab10_train.csv\", header=True, schema=StructType(\n",
    "                            [StructField(\"user_id\", IntegerType(), True),\n",
    "                             StructField(\"item_id\", IntegerType(), True),\n",
    "                             StructField(\"purchase\", DoubleType(), True)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  74107|     0.0|\n",
      "|   1654|  89249|     0.0|\n",
      "|   1654|  99982|     0.0|\n",
      "|   1654|  89901|     0.0|\n",
      "|   1654| 100504|     0.0|\n",
      "+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "true = spark.read.csv(\"lab10_true.csv\", header=True, schema=StructType(\n",
    "                            [StructField(\"user_id\", IntegerType(), True),\n",
    "                             StructField(\"item_id\", IntegerType(), True),\n",
    "                             StructField(\"purchase\", DoubleType(), True)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  94814|     0.0|\n",
      "|   1654|  93629|     0.0|\n",
      "|   1654|   9980|     0.0|\n",
      "|   1654|  95099|     0.0|\n",
      "|   1654|  11265|     0.0|\n",
      "|   1654|  88896|     0.0|\n",
      "|   1654|  67740|     0.0|\n",
      "|   1654|  74271|     0.0|\n",
      "|   1654|  99871|     0.0|\n",
      "|   1654|  78570|     0.0|\n",
      "|   1654|  71942|     0.0|\n",
      "|   1654|  74367|     0.0|\n",
      "|   1654|  98628|     0.0|\n",
      "|   1654|  95887|     0.0|\n",
      "|   1654|  77795|     0.0|\n",
      "|   1654|  75152|     0.0|\n",
      "|   1654|  74905|     0.0|\n",
      "|   1654|   9068|     0.0|\n",
      "|   1654|  72954|     0.0|\n",
      "|   1654| 102431|     0.0|\n",
      "+-------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "train_rdd = sc.textFile(\"/labs/lab10data/lab10_train.csv\")\n",
    "header = train_rdd.first()\n",
    "train_rdd = train_rdd.filter(lambda x: x != header)\n",
    "train_rdd = train_rdd.map(lambda l: l.split(',')).map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))\n",
    "\n",
    "def normalize_predictions(pred, coef=0.0007):\n",
    "    if pred < coef:\n",
    "        return 0.0\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=1654, product=94814, rating=0.0),\n",
       " Rating(user=1654, product=93629, rating=0.0),\n",
       " Rating(user=1654, product=9980, rating=0.0),\n",
       " Rating(user=1654, product=95099, rating=0.0),\n",
       " Rating(user=1654, product=11265, rating=0.0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rdd = sc.textFile(\"lab10_true.csv\")\n",
    "header = test_rdd.first()\n",
    "test_rdd = test_rdd.filter(lambda x: x != header)\n",
    "test_rdd = test_rdd.map(lambda l: l.split(',')).map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))\n",
    "test_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "unlabeled_test_rdd = test_rdd.map(lambda x: (x[0], x[1])) # holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tr_rdd0, labeled_val_rdd0 = train_rdd.randomSplit([95, 5], seed=100) # seed matters!\n",
    "tr_rdd1, labeled_val_rdd1 = train_rdd.randomSplit([95, 5], seed=1000) # seed matters!\n",
    "tr_rdd2, labeled_val_rdd2 = train_rdd.randomSplit([95, 5], seed=2)\n",
    "\n",
    "\n",
    "unlabeled_val_rdd0 = labeled_val_rdd0.map(lambda x: (x[0], x[1])) # holdout\n",
    "unlabeled_val_rdd1 = labeled_val_rdd1.map(lambda x: (x[0], x[1])) # holdout\n",
    "unlabeled_val_rdd2 = labeled_val_rdd2.map(lambda x: (x[0], x[1])) # holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = tr_rdd.toDF(schema=StructType([StructField(\"user_id\", IntegerType(), True),\n",
    "                           StructField(\"item_id\", IntegerType(), True),\n",
    "                           StructField(\"purchase\", DoubleType(), True)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "valid = labeled_val_rdd.toDF(schema=StructType([StructField(\"user_id\", IntegerType(), True),\n",
    "                           StructField(\"item_id\", IntegerType(), True),\n",
    "                           StructField(\"purchase\", DoubleType(), True)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  74107|     0.0|\n",
      "|   1654|  89249|     0.0|\n",
      "|   1654|  99982|     0.0|\n",
      "|   1654|  89901|     0.0|\n",
      "|   1654| 100504|     0.0|\n",
      "+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4528926"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  66187|     0.0|\n",
      "|   1654|  84350|     0.0|\n",
      "|   1654|  83584|     0.0|\n",
      "|   1654| 100715|     0.0|\n",
      "|   1654|  74660|     0.0|\n",
      "+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valid.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ALS RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search before training looked like this:\n",
    "\n",
    "```python\n",
    "%%time\n",
    "seed = 2L\n",
    "iterations = [10,15,25]\n",
    "regularization_parameters = [0.01, 0.1, 0.5]\n",
    "ranks = [3,5,8,10]\n",
    "errors = [0, 0, 0]\n",
    "err = 0\n",
    "tolerance = 0.02\n",
    "\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "best_iteration = -1\n",
    "for it in iterations:\n",
    "    for rank in ranks:\n",
    "        for reg in regularization_parameters:\n",
    "            model = ALS.trainImplicit(training_RDD, rank, seed=seed, iterations=it, lambda_=reg)\n",
    "            predictions = model.predictAll(validation_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "            rates_and_preds = validation_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "\n",
    "            pred_labels = rates_and_preds.map(lambda r: (normalize_predictions(r[1][1]),r[1][0]))\n",
    "            metrics = BinaryClassificationMetrics(pred_labels)\n",
    "\n",
    "            auc = metrics.areaUnderROC\n",
    "\n",
    "            print 'Iter: {}, rank: {}, lambda: {} --- ROC AUC: {}'.format(it, rank, reg, auc)\n",
    "\n",
    "            error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "\n",
    "            print 'For rank %s the RMSE is %s' % (rank, error)\n",
    "            print ''\n",
    "```   \n",
    "         \n",
    "#### Output:\n",
    "\n",
    "```\n",
    "Iter: 10, rank: 3, lambda: 0.01 --- ROC AUC: 0.731639934709\n",
    "For rank 3 the RMSE is 0.0460338345603\n",
    "\n",
    "Iter: 10, rank: 3, lambda: 0.1 --- ROC AUC: 0.734514795168\n",
    "For rank 3 the RMSE is 0.0460816121302\n",
    "\n",
    "Iter: 10, rank: 3, lambda: 0.5 --- ROC AUC: 0.777328607903\n",
    "For rank 3 the RMSE is 0.0464224929801\n",
    "\n",
    "Iter: 10, rank: 5, lambda: 0.01 --- ROC AUC: 0.729200432866\n",
    "For rank 5 the RMSE is 0.0460663663214\n",
    "\n",
    "Iter: 10, rank: 5, lambda: 0.1 --- ROC AUC: 0.737563830091\n",
    "For rank 5 the RMSE is 0.0460465162737\n",
    "\n",
    "Iter: 10, rank: 5, lambda: 0.5 --- ROC AUC: 0.782990899661\n",
    "For rank 5 the RMSE is 0.0463852176211\n",
    "\n",
    "Iter: 10, rank: 8, lambda: 0.01 --- ROC AUC: 0.729080948902\n",
    "For rank 8 the RMSE is 0.0461674935539\n",
    "\n",
    "Iter: 10, rank: 8, lambda: 0.1 --- ROC AUC: 0.74310150625\n",
    "For rank 8 the RMSE is 0.0460820215917\n",
    "\n",
    "Iter: 10, rank: 8, lambda: 0.5 --- ROC AUC: 0.777392459025\n",
    "For rank 8 the RMSE is 0.0463953593337\n",
    "\n",
    "Iter: 10, rank: 10, lambda: 0.01 --- ROC AUC: 0.727449790168\n",
    "For rank 10 the RMSE is 0.0462061898834\n",
    "\n",
    "Iter: 10, rank: 10, lambda: 0.1 --- ROC AUC: 0.745124098603\n",
    "For rank 10 the RMSE is 0.0460618873368\n",
    "\n",
    "Iter: 10, rank: 10, lambda: 0.5 --- ROC AUC: 0.773666927634\n",
    "For rank 10 the RMSE is 0.0463838665147\n",
    "\n",
    "Iter: 15, rank: 3, lambda: 0.01 --- ROC AUC: 0.733625236548\n",
    "For rank 3 the RMSE is 0.0460081756296\n",
    "\n",
    "Iter: 15, rank: 3, lambda: 0.1 --- ROC AUC: 0.741792649635\n",
    "For rank 3 the RMSE is 0.0460386549206\n",
    "\n",
    "Iter: 15, rank: 3, lambda: 0.5 --- ROC AUC: 0.776499441223\n",
    "For rank 3 the RMSE is 0.0464180265494\n",
    "\n",
    "Iter: 15, rank: 5, lambda: 0.01 --- ROC AUC: 0.72852852113\n",
    "For rank 5 the RMSE is 0.046075054415\n",
    "\n",
    "Iter: 15, rank: 5, lambda: 0.1 --- ROC AUC: 0.741907337317\n",
    "For rank 5 the RMSE is 0.0460478751819\n",
    "\n",
    "Iter: 15, rank: 5, lambda: 0.5 --- ROC AUC: 0.776469790115\n",
    "For rank 5 the RMSE is 0.0463898646245\n",
    "\n",
    "Iter: 15, rank: 8, lambda: 0.01 --- ROC AUC: 0.727741522873\n",
    "For rank 8 the RMSE is 0.0461247319256\n",
    "\n",
    "Iter: 15, rank: 8, lambda: 0.1 --- ROC AUC: 0.748216767839\n",
    "For rank 8 the RMSE is 0.046062139977\n",
    "\n",
    "Iter: 15, rank: 8, lambda: 0.5 --- ROC AUC: 0.776448318741\n",
    "For rank 8 the RMSE is 0.0463956123179\n",
    "\n",
    "Iter: 15, rank: 10, lambda: 0.01 --- ROC AUC: 0.719892506267\n",
    "For rank 10 the RMSE is 0.0462075234346\n",
    "\n",
    "Iter: 15, rank: 10, lambda: 0.1 --- ROC AUC: 0.739951904031\n",
    "For rank 10 the RMSE is 0.0460849725144\n",
    "\n",
    "Iter: 15, rank: 10, lambda: 0.5 --- ROC AUC: 0.774371417949\n",
    "For rank 10 the RMSE is 0.0463880383276\n",
    "\n",
    "Iter: 25, rank: 3, lambda: 0.01 --- ROC AUC: 0.735054175047\n",
    "For rank 3 the RMSE is 0.0460087478259\n",
    "\n",
    "Iter: 25, rank: 3, lambda: 0.1 --- ROC AUC: 0.743852812528\n",
    "For rank 3 the RMSE is 0.0460401572356\n",
    "\n",
    "Iter: 25, rank: 3, lambda: 0.5 --- ROC AUC: 0.765828539125\n",
    "For rank 3 the RMSE is 0.0464590431011\n",
    "\n",
    "Iter: 25, rank: 5, lambda: 0.01 --- ROC AUC: 0.73399945951\n",
    "For rank 5 the RMSE is 0.0460800511798\n",
    "\n",
    "Iter: 25, rank: 5, lambda: 0.1 --- ROC AUC: 0.744554115978\n",
    "For rank 5 the RMSE is 0.0460531775825\n",
    "\n",
    "Iter: 25, rank: 5, lambda: 0.5 --- ROC AUC: 0.776661188388\n",
    "For rank 5 the RMSE is 0.0464027118638\n",
    "\n",
    "Iter: 25, rank: 8, lambda: 0.01 --- ROC AUC: 0.732513689155\n",
    "For rank 8 the RMSE is 0.0460869728764\n",
    "\n",
    "Iter: 25, rank: 8, lambda: 0.1 --- ROC AUC: 0.756796014108\n",
    "For rank 8 the RMSE is 0.04601012765\n",
    "\n",
    "Iter: 25, rank: 8, lambda: 0.5 --- ROC AUC: 0.776545226641\n",
    "For rank 8 the RMSE is 0.04639385306\n",
    "\n",
    "Iter: 25, rank: 10, lambda: 0.01 --- ROC AUC: 0.714334654793\n",
    "For rank 10 the RMSE is 0.0461831036839\n",
    "\n",
    "Iter: 25, rank: 10, lambda: 0.1 --- ROC AUC: 0.746096587359\n",
    "For rank 10 the RMSE is 0.0460914581093\n",
    "\n",
    "Iter: 25, rank: 10, lambda: 0.5 --- ROC AUC: 0.775843093541\n",
    "For rank 10 the RMSE is 0.0463948288111\n",
    "\n",
    "CPU times: user 7.14 s, sys: 2.55 s, total: 9.69 s\n",
    "Wall time: 1h 36min 19s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS #: 0, Iter: 10, rank: 5, lambda: 0.5 --- ROC AUC: 0.831064888722\n",
      "ALS #: 1, Iter: 10, rank: 5, lambda: 0.5 --- ROC AUC: 0.825141751005\n",
      "ALS #: 2, Iter: 10, rank: 5, lambda: 0.5 --- ROC AUC: 0.805082927758\n"
     ]
    }
   ],
   "source": [
    "# Training the best model\n",
    "\n",
    "seed = [10,100,1000]\n",
    "iterations = 10\n",
    "regularization_parameter = 0.5\n",
    "rank = 5\n",
    "\n",
    "als_train0, als_train1, als_train2 = [ALS.trainImplicit(tr_rdd, rank, seed=s, iterations=iterations,\n",
    "                                                       lambda_=regularization_parameter) \\\n",
    "                                      for s, tr_rdd in zip(seed, [tr_rdd0, tr_rdd1, tr_rdd2])]\n",
    "\n",
    "for i in range(3):\n",
    "    als_train = globals()['als_train{}'.format(str(i))]\n",
    "    labeled_val_rdd = globals()['labeled_val_rdd{}'.format(str(i))]\n",
    "    unlabeled_val_rdd = globals()['unlabeled_val_rdd{}'.format(str(i))]\n",
    "\n",
    "    predictions_val = als_train.predictAll(unlabeled_val_rdd).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "    labels_predictions_val = labeled_val_rdd.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions_val)\n",
    "\n",
    "    pred_labels = labels_predictions_val.map(lambda r: (r[1][1],r[1][0]))\n",
    "\n",
    "    metrics = BinaryClassificationMetrics(pred_labels)\n",
    "\n",
    "    auc = metrics.areaUnderROC\n",
    "\n",
    "    print 'ALS #: {}, Iter: {}, rank: {}, lambda: {} --- ROC AUC: {}' \\\n",
    "          .format(i, iterations, rank, regularization_parameter, auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 10, rank: 5, lambda: 0.5 --- ROC AUC: 0.823397319514\n"
     ]
    }
   ],
   "source": [
    "# Final test: ALS #0\n",
    "\n",
    "predictions_test = als_train0.predictAll(unlabeled_test_rdd).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "labels_predictions_test = test_rdd.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions_test)\n",
    "\n",
    "pred_labels_test = labels_predictions_test.map(lambda r: (r[1][1],r[1][0]))\n",
    "\n",
    "metrics = BinaryClassificationMetrics(pred_labels_test)\n",
    "\n",
    "auc_test = metrics.areaUnderROC\n",
    "\n",
    "print 'Iter: {}, rank: {}, lambda: {} --- ROC AUC: {}'.format(iterations, rank, regularization_parameter, auc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 10, rank: 5, lambda: 0.5 --- ROC AUC: 0.814314117531\n"
     ]
    }
   ],
   "source": [
    "# Final test: ALS #1\n",
    "\n",
    "predictions_test = als_train1.predictAll(unlabeled_test_rdd).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "labels_predictions_test = test_rdd.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions_test)\n",
    "\n",
    "pred_labels_test = labels_predictions_test.map(lambda r: (r[1][1],r[1][0]))\n",
    "\n",
    "metrics = BinaryClassificationMetrics(pred_labels_test)\n",
    "\n",
    "auc_test = metrics.areaUnderROC\n",
    "\n",
    "print 'Iter: {}, rank: {}, lambda: {} --- ROC AUC: {}'.format(iterations, rank, regularization_parameter, auc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 10, rank: 5, lambda: 0.5 --- ROC AUC: 0.80361851881\n"
     ]
    }
   ],
   "source": [
    "# Final test: ALS #2\n",
    "\n",
    "predictions_test = als_train2.predictAll(unlabeled_test_rdd).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "labels_predictions_test = test_rdd.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions_test)\n",
    "\n",
    "pred_labels_test = labels_predictions_test.map(lambda r: (r[1][1],r[1][0]))\n",
    "\n",
    "metrics = BinaryClassificationMetrics(pred_labels_test)\n",
    "\n",
    "auc_test = metrics.areaUnderROC\n",
    "\n",
    "print 'Iter: {}, rank: {}, lambda: {} --- ROC AUC: {}'.format(iterations, rank, regularization_parameter, auc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Bagging\n",
    "\n",
    "preds = unlabeled_test_rdd.map(lambda r: ((r[0], r[1]), 0.0))\n",
    "\n",
    "for i in range(3):\n",
    "    als_train = globals()['als_train{}'.format(str(i))]\n",
    "    \n",
    "    preds = preds.join(als_train.predictAll(unlabeled_test_rdd) \\\n",
    "                 .map(lambda r: ((r[0], r[1]), r[2])))\n",
    "    \n",
    "    preds = preds.map(lambda r: (r[0], float(r[1][0]) + float(r[1][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 10, rank: 5, lambda: 0.5 --- ROC AUC: 0.820694695724\n"
     ]
    }
   ],
   "source": [
    "labels_predictions_test = test_rdd.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(preds)\n",
    "\n",
    "pred_labels_test = labels_predictions_test.map(lambda r: (r[1][1],r[1][0]))\n",
    "\n",
    "metrics = BinaryClassificationMetrics(pred_labels_test)\n",
    "\n",
    "auc_test = metrics.areaUnderROC\n",
    "\n",
    "print 'Iter: {}, rank: {}, lambda: {} --- ROC AUC: {}'.format(iterations, rank, regularization_parameter, auc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Leaderboard overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Th: 9.99999974738e-05 --- ROC AUC: 0.802632181711\n",
      "Th: 0.000199999994948 --- ROC AUC: 0.804973112584\n",
      "Th: 0.000300000014249 --- ROC AUC: 0.806815220176\n",
      "Th: 0.000399999989895 --- ROC AUC: 0.808580293232\n",
      "Th: 0.000500000023749 --- ROC AUC: 0.810006263879\n",
      "Th: 0.000600000028498 --- ROC AUC: 0.81084930304\n",
      "Th: 0.000699999975041 --- ROC AUC: 0.811466458439\n",
      "Th: 0.00079999997979 --- ROC AUC: 0.812480345742\n",
      "Th: 0.00089999998454 --- ROC AUC: 0.813469812561\n",
      "Th: 0.0010000000475 --- ROC AUC: 0.813855603908\n"
     ]
    }
   ],
   "source": [
    "preds.cache()\n",
    "\n",
    "thresholds = np.linspace(0.0001, 0.001, 10, dtype='float32')\n",
    "for th in thresholds:\n",
    "    # preds \n",
    "    labels_predictions_test = test_rdd.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(preds)\n",
    "    pred_labels_test = labels_predictions_test.map(lambda r: (normalize_predictions(r[1][1], coef=th), r[1][0]))\n",
    "    metrics = BinaryClassificationMetrics(pred_labels_test)\n",
    "    auc_test = metrics.areaUnderROC\n",
    "    print 'Th: {} --- ROC AUC: {}'.format(th, auc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Th: 0.0010000000475 --- ROC AUC: 0.813855603908\n",
      "Th: 0.00144444440957 --- ROC AUC: 0.815476906776\n",
      "Th: 0.00188888888806 --- ROC AUC: 0.816979569086\n",
      "Th: 0.00233333325014 --- ROC AUC: 0.817385465429\n",
      "Th: 0.00277777784504 --- ROC AUC: 0.817350554678\n",
      "Th: 0.00322222220711 --- ROC AUC: 0.8169363357\n",
      "Th: 0.00366666656919 --- ROC AUC: 0.816029337946\n",
      "Th: 0.00411111116409 --- ROC AUC: 0.815498556857\n",
      "Th: 0.00455555552617 --- ROC AUC: 0.813430678387\n",
      "Th: 0.00499999988824 --- ROC AUC: 0.812065229264\n"
     ]
    }
   ],
   "source": [
    "thresholds = np.linspace(0.001, 0.005, 10, dtype='float32')\n",
    "for th in thresholds:\n",
    "    pred_labels_test = labels_predictions_test.map(lambda r: (normalize_predictions(r[1][1], coef=th), r[1][0]))\n",
    "    metrics = BinaryClassificationMetrics(pred_labels_test)\n",
    "    auc_test = metrics.areaUnderROC\n",
    "    print 'Th: {} --- ROC AUC: {}'.format(th, auc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Gradient Boosting on other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stage 1. Processing items dataset into items_genres and items_year\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer, RegexTokenizer, HashingTF, VectorAssembler\n",
    "from pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "\n",
    "year_as_int = udf(lambda v: int(v) if v is not None else None, IntegerType())\n",
    "\n",
    "items = spark.read.csv(\"/labs/lab10data/lab10_items.csv\", header=True, sep='\\t')\n",
    "items_genres_years = items.select('item_id', 'genres', 'year')\n",
    "items_genres_years = items_genres_years.na.fill({'genres': u'_'})\n",
    "items_genres_years = items_genres_years.na.fill({'year': u'1899'})\n",
    "items_genres_years = items_genres_years.withColumn('year', items_genres_years.year.cast(IntegerType()))\n",
    "\n",
    "tokenizer = RegexTokenizer(inputCol=\"genres\", outputCol=\"genre_tokens\", gaps=False, pattern=ur\"[_А-Яа-яёЁ]+\", toLowercase=False)\n",
    "\n",
    "items_genres_years_tk = tokenizer.transform(items_genres_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"genre_tokens\", outputCol=\"genre_vector\")\n",
    "cv_model = cv.fit(items_genres_years_tk)\n",
    "items_features_vec = cv_model.transform(items_genres_years_tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----+------------+---------------+\n",
      "|item_id| genres|year|genre_tokens|   genre_vector|\n",
      "+-------+-------+----+------------+---------------+\n",
      "|  65667|Эротика|2013|   [Эротика]|(96,[22],[1.0])|\n",
      "|  65669|Эротика|2011|   [Эротика]|(96,[22],[1.0])|\n",
      "|  65668|Эротика|2011|   [Эротика]|(96,[22],[1.0])|\n",
      "|  65671|Эротика|2011|   [Эротика]|(96,[22],[1.0])|\n",
      "|  65670|Эротика|2010|   [Эротика]|(96,[22],[1.0])|\n",
      "+-------+-------+----+------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items_features_vec.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[item_id: int, item_count: bigint, item_row_id: bigint]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# History vector\n",
    "# First we reduce our dataset to 1000 most popular films\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "items_count = train.groupBy('item_id').count().withColumnRenamed('count', 'item_count')\n",
    "items_desc_count = items_count.orderBy(items_count.item_count.desc()).limit(1000) \\\n",
    "                              .withColumn(\"item_row_id\", monotonically_increasing_id())\n",
    "\n",
    "items_desc_count.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_desc_count.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------+\n",
      "|item_id|item_count|item_row_id|\n",
      "+-------+----------+-----------+\n",
      "|  98971|      1351|          0|\n",
      "|  71740|      1349|          1|\n",
      "|   5117|      1348|          2|\n",
      "|   7618|      1348|          3|\n",
      "|  98069|      1348|          4|\n",
      "+-------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items_desc_count.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, item_row_id: bigint, purchase: double]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_truncated = train.join(items_desc_count, 'item_id', 'inner').select('user_id', 'item_row_id', 'purchase')\n",
    "train_truncated.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, item_row_id: bigint, purchase: double]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_truncated = valid.join(items_desc_count, 'item_id', 'inner').select('user_id', 'item_id', 'purchase')\n",
    "train_truncated.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Now we build up a history vector\n",
    "\n",
    "as_ml = udf(lambda v: v.asML() if v is not None else None, VectorUDT())\n",
    "\n",
    "from_ml = udf(lambda v: Vectors.fromML(v) if v is not None else None)\n",
    "\n",
    "train_matrix = train_truncated.rdd.map(lambda r: MatrixEntry(r[0], r[1], r[2]))\n",
    "\n",
    "train_matrix = CoordinateMatrix(train_matrix)\n",
    "\n",
    "train_row_mat_i = train_matrix.toIndexedRowMatrix()\n",
    "\n",
    "train_mat_df = train_row_mat_i.rows.toDF().withColumnRenamed('index', 'user_id') \\\n",
    "                                    .withColumnRenamed('vector', 'history_vec') \\\n",
    "                                    .withColumn(\"history_vec\", as_ml(\"history_vec\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|user_id|         history_vec|\n",
      "+-------+--------------------+\n",
      "| 922400|(1000,[1,2,3,5,7,...|\n",
      "| 940600|(1000,[0,1,2,3,5,...|\n",
      "| 866400|(1000,[0,1,3,4,5,...|\n",
      "| 857000|(1000,[1,2,4,5,7,...|\n",
      "| 899200|(1000,[0,1,3,4,5,...|\n",
      "| 879401|(1000,[0,1,2,3,5,...|\n",
      "| 749801|(1000,[0,1,5,6,7,...|\n",
      "| 792601|(1000,[0,1,2,3,5,...|\n",
      "| 905401|(1000,[0,2,4,5,7,...|\n",
      "| 905201|(1000,[1,3,4,5,6,...|\n",
      "+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_mat_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, item_id: int, history_vec: vector, genre_vector: vector, year: int, purchase: double]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Joining everything together\n",
    "\n",
    "train_joined = train.join(train_mat_df, 'user_id', 'left') \\\n",
    "                    .join(items_features_vec, 'item_id', 'left') \\\n",
    "                    .select('user_id', 'item_id', 'history_vec', 'genre_vector', 'year', 'purchase')\n",
    "        \n",
    "valid_joined = valid.join(train_mat_df, 'user_id', 'left') \\\n",
    "                    .join(items_features_vec, 'item_id', 'left') \\\n",
    "                    .select('user_id', 'item_id', 'history_vec', 'genre_vector', 'year', 'purchase')\n",
    "\n",
    "test_joined = true.join(train_mat_df, 'user_id', 'left') \\\n",
    "                    .join(items_features_vec, 'item_id', 'left') \\\n",
    "                    .select('user_id', 'item_id', 'history_vec', 'genre_vector', 'year', 'purchase')\n",
    "        \n",
    "\n",
    "train_joined.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Building a single vector\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=[\"history_vec\", \"genre_vector\", 'year'], outputCol=\"features\")\n",
    "# vec_assembler = VectorAssembler(inputCols=[\"history_vec\", \"genre_vector\"], outputCol=\"features\")\n",
    "train_joined = vec_assembler.transform(train_joined)\n",
    "valid_joined = vec_assembler.transform(valid_joined)\n",
    "test_joined = vec_assembler.transform(test_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_fm_dataset = train_joined.select('purchase', 'features') \\\n",
    "                               .withColumnRenamed('purchase', 'label')\n",
    "valid_fm_dataset = valid_joined.select('purchase', 'features') \\\n",
    "                               .withColumnRenamed('purchase', 'label')\n",
    "    \n",
    "test_fm_dataset = test_joined.select('purchase', 'features') \\\n",
    "                               .withColumnRenamed('purchase', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fm_dataset.where(train_fm_dataset.features.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "best_gbt = GBTRegressor(maxMemoryInMB=10240, \n",
    "                        maxIter=50,\n",
    "                        subsamplingRate=1.0,\n",
    "                        maxDepth=9,\n",
    "                        featuresCol=\"features\",\n",
    "                        labelCol=\"label\",\n",
    "                        predictionCol=\"rawPrediction\")\n",
    "fitted_gbt = best_gbt.fit(train_fm_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "transformed = fitted_gbt.transform(valid_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8422141757567635"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol=\"purchase\")\n",
    "score = evaluator.evaluate(transformed)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## subsampling: 1.0; maxDepth: 5\n",
    "# 10 iter: 0.7671101003103943\n",
    "# 15 iter: 0.7810317725957958\n",
    "# 30 iter: 0.7999755632153878\n",
    "\n",
    "## subsampling: 1.0; maxDepth: 6\n",
    "# 30 iter: 0.811694625573447\n",
    "\n",
    "## subsampling: 1.0; maxDepth: 7\n",
    "# 30 iter: 0.8278831951589867\n",
    "\n",
    "## subsampling: 1.0; maxDepth: 8\n",
    "# 30 iter: 0.822975757925702\n",
    "# 40 iter: 0.836309864699397\n",
    "# 50 iter: 0.8366130295710614\n",
    "\n",
    "## subsampling: 1.0; maxDepth: 9\n",
    "# 50 iter: 0.8425874387927447\n",
    "# 60 iter: 0.8400094072324349 (year as float)\n",
    "\n",
    "## subsampling: 1.0; maxDepth: 10\n",
    "# 60 iter: 0.8345177147366496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "postprocess = udf(lambda v: normalize_predictions(v), DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "transformed_test = fitted_gbt.transform(test_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "transformed_test = transformed_test.withColumn('rawPrediction', postprocess(transformed_test['rawPrediction'])) \\\n",
    "                                   .withColumnRenamed('purchase', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+--------------------+----+-----+--------------------+--------------------+\n",
      "|user_id|item_id|         history_vec|        genre_vector|year|label|            features|       rawPrediction|\n",
      "+-------+-------+--------------------+--------------------+----+-----+--------------------+--------------------+\n",
      "| 782482|  10129|(1000,[0,1,2,3,4,...|(96,[0,1,4,39],[1...|2013|  0.0|(1097,[22,457,508...|0.002464763166527495|\n",
      "| 892108|  10129|(1000,[0,2,3,4,5,...|(96,[0,1,4,39],[1...|2013|  0.0|(1097,[187,201,22...|0.013839999679402254|\n",
      "| 892140|  10129|(1000,[0,4,5,6,7,...|(96,[0,1,4,39],[1...|2013|  0.0|(1097,[1000,1001,...|7.037651117619899E-4|\n",
      "| 879985|  10129|(1000,[0,4,6,9,12...|(96,[0,1,4,39],[1...|2013|  0.0|(1097,[715,780,10...|7.037651117619899E-4|\n",
      "| 905741|  10129|(1000,[1,3,4,5,6,...|(96,[0,1,4,39],[1...|2013|  0.0|(1097,[339,701,77...|  0.0080496050143013|\n",
      "| 927035|  10129|(1000,[0,1,2,3,5,...|(96,[0,1,4,39],[1...|2013|  0.0|(1097,[1000,1001,...|7.037651117619899E-4|\n",
      "| 842257|  10129|(1000,[0,1,5,8,9,...|(96,[0,1,4,39],[1...|2013|  0.0|(1097,[339,483,96...| 0.01407310494308554|\n",
      "| 899903|  10129|(1000,[1,2,4,5,7,...|(96,[0,1,4,39],[1...|2013|  0.0|(1097,[1000,1001,...|7.037651117619899E-4|\n",
      "| 939341|  10129|(1000,[1,2,3,4,5,...|(96,[0,1,4,39],[1...|2013|  0.0|(1097,[1000,1001,...|7.037651117619899E-4|\n",
      "| 898812|  10129|(1000,[0,2,7,8,10...|(96,[0,1,4,39],[1...|2013|  0.0|(1097,[1000,1001,...|7.037651117619899E-4|\n",
      "| 936557|  10129|(1000,[1,2,3,5,7,...|(96,[0,1,4,39],[1...|2013|  0.0|(1097,[1000,1001,...|7.037651117619899E-4|\n",
      "| 792601|  10129|(1000,[0,1,2,3,5,...|(96,[0,1,4,39],[1...|2013|  0.0|(1097,[1000,1001,...|7.037651117619899E-4|\n",
      "| 833424|  10129|(1000,[0,1,2,3,4,...|(96,[0,1,4,39],[1...|2013|  0.0|(1097,[812,1000,1...|7.037651117619899E-4|\n",
      "| 841856|  10129|(1000,[0,1,2,4,6,...|(96,[0,1,4,39],[1...|2013|  0.0|(1097,[1000,1001,...|7.037651117619899E-4|\n",
      "| 865692|  10129|(1000,[0,1,2,3,4,...|(96,[0,1,4,39],[1...|2013|  0.0|(1097,[362,1000,1...|7.037651117619899E-4|\n",
      "| 837850|  10129|(1000,[0,1,2,3,5,...|(96,[0,1,4,39],[1...|2013|  0.0|(1097,[1000,1001,...|7.037651117619899E-4|\n",
      "| 848634|  10129|(1000,[0,3,5,9,12...|(96,[0,1,4,39],[1...|2013|  0.0|(1097,[1000,1001,...|7.037651117619899E-4|\n",
      "| 915704|  10129|(1000,[1,2,3,5,7,...|(96,[0,1,4,39],[1...|2013|  0.0|(1097,[1000,1001,...|7.037651117619899E-4|\n",
      "| 882394|  10129|(1000,[0,1,3,5,7,...|(96,[0,1,4,39],[1...|2013|  0.0|(1097,[1000,1001,...|7.037651117619899E-4|\n",
      "| 939159|  10129|(1000,[0,1,2,3,4,...|(96,[0,1,4,39],[1...|2013|  0.0|(1097,[1000,1001,...|7.037651117619899E-4|\n",
      "+-------+-------+--------------------+--------------------+----+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8471244793902535"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "score2 = evaluator.evaluate(transformed_test)\n",
    "score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## subsampling: 1.0; maxDepth: 8\n",
    "# 30 iter: 0.8249368397566905 -- better than on validation!\n",
    "# 40 iter: 0.838257894132567\n",
    "\n",
    "## subsampling: 1.0; maxDepth: 9\n",
    "# 50 iter: 0.8460460801596608\n",
    "# 60 iter: 0.8427943930567464 (year as float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "transformed_test.select('user_id', 'item_id', 'rawPrediction') \\\n",
    "                .withColumnRenamed('rawPrediction', 'purchase') \\\n",
    "                .orderBy(['user_id', 'item_id']) \\\n",
    "                .coalesce(1) \\\n",
    "                .write \\\n",
    "                .format(\"com.databricks.spark.csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .save('lab10s.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## LibFM preparation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4780428"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fm_dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, features: vector]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fm_dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "LIBFM_PATH = '/data/home/anton.pilipenko/libfm-1.42.src/bin/libFM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "\n",
    "data_fm_tr = train_fm_dataset.rdd.map(lambda r: LabeledPoint(r[0],Vectors.fromML(r[1])))\n",
    "data_fm_val = valid_fm_dataset.rdd.map(lambda r: LabeledPoint(r[0],Vectors.fromML(r[1])))\n",
    "\n",
    "MLUtils.saveAsLibSVMFile(data_fm_tr, \"train_fm.data\")\n",
    "MLUtils.saveAsLibSVMFile(data_fm_val, \"valid_fm.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_fm_test = test_fm_dataset.rdd.map(lambda r: LabeledPoint(r[0],Vectors.fromML(r[1]))).coalesce(1)\n",
    "MLUtils.saveAsLibSVMFile(data_fm_test, \"test_fm.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, (597,[500,501,504,539,596],[1.0,1.0,1.0,1.0,0.998016856718])),\n",
       " LabeledPoint(0.0, (597,[500,501,504,539,596],[1.0,1.0,1.0,1.0,0.998016856718])),\n",
       " LabeledPoint(0.0, (597,[500,501,504,539,596],[1.0,1.0,1.0,1.0,0.998016856718])),\n",
       " LabeledPoint(0.0, (597,[500,501,504,539,596],[1.0,1.0,1.0,1.0,0.998016856718])),\n",
       " LabeledPoint(0.0, (597,[500,501,504,539,596],[1.0,1.0,1.0,1.0,0.998016856718])),\n",
       " LabeledPoint(0.0, (597,[500,501,504,539,596],[1.0,1.0,1.0,1.0,0.998016856718])),\n",
       " LabeledPoint(0.0, (597,[500,501,504,539,596],[1.0,1.0,1.0,1.0,0.998016856718])),\n",
       " LabeledPoint(0.0, (597,[500,501,504,539,596],[1.0,1.0,1.0,1.0,0.998016856718])),\n",
       " LabeledPoint(0.0, (597,[500,501,504,539,596],[1.0,1.0,1.0,1.0,0.998016856718])),\n",
       " LabeledPoint(0.0, (597,[213,500,501,504,539,596],[1.0,1.0,1.0,1.0,1.0,0.998016856718])),\n",
       " LabeledPoint(0.0, (597,[500,501,504,539,596],[1.0,1.0,1.0,1.0,0.998016856718])),\n",
       " LabeledPoint(0.0, (597,[500,501,504,539,596],[1.0,1.0,1.0,1.0,0.998016856718])),\n",
       " LabeledPoint(0.0, (597,[0,119,253,266,489,500,501,504,539,596],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.998016856718])),\n",
       " LabeledPoint(0.0, (597,[368,500,501,504,539,596],[1.0,1.0,1.0,1.0,1.0,0.998016856718])),\n",
       " LabeledPoint(0.0, (597,[500,501,504,539,596],[1.0,1.0,1.0,1.0,0.998016856718]))]"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fm_tr.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "libFM\n",
      "  Version: 1.4.2\n",
      "  Author:  Steffen Rendle, srendle@libfm.org\n",
      "  WWW:     http://www.libfm.org/\n",
      "This program comes with ABSOLUTELY NO WARRANTY; for details see license.txt.\n",
      "This is free software, and you are welcome to redistribute it under certain\n",
      "conditions; for details see license.txt.\n",
      "----------------------------------------------------------------------------\n",
      "Loading train...\t\n",
      "has x = 0\n",
      "has xt = 1\n",
      "num_rows=4780428\tnum_values=24695766\tnum_features=598\tmin_target=0\tmax_target=1\n",
      "Loading test... \t\n",
      "has x = 0\n",
      "has xt = 1\n",
      "num_rows=252196\tnum_values=1302963\tnum_features=598\tmin_target=0\tmax_target=1\n",
      "#relations: 0\n",
      "Loading meta data...\t\n",
      "#Iter=  0\tTrain=0.997829\tTest=0.997831\tTest(ll)=0.0784925\n",
      "#Iter=  1\tTrain=0.997833\tTest=0.997835\tTest(ll)=0.0632365\n",
      "#Iter=  2\tTrain=0.997833\tTest=0.997835\tTest(ll)=0.0534096\n",
      "#Iter=  3\tTrain=0.997833\tTest=0.997839\tTest(ll)=0.0465388\n",
      "#Iter=  4\tTrain=0.997833\tTest=0.997839\tTest(ll)=0.0414494\n",
      "#Iter=  5\tTrain=0.997833\tTest=0.997839\tTest(ll)=0.0375216\n",
      "#Iter=  6\tTrain=0.997833\tTest=0.997839\tTest(ll)=0.0343861\n",
      "#Iter=  7\tTrain=0.997834\tTest=0.997839\tTest(ll)=0.0318224\n",
      "#Iter=  8\tTrain=0.997833\tTest=0.997839\tTest(ll)=0.0296834\n",
      "#Iter=  9\tTrain=0.997834\tTest=0.997839\tTest(ll)=0.0278718\n",
      "#Iter= 10\tTrain=0.997834\tTest=0.997839\tTest(ll)=0.0263163\n",
      "#Iter= 11\tTrain=0.997834\tTest=0.997839\tTest(ll)=0.0249644\n",
      "#Iter= 12\tTrain=0.997833\tTest=0.997839\tTest(ll)=0.0237788\n",
      "#Iter= 13\tTrain=0.997833\tTest=0.997839\tTest(ll)=0.0227308\n",
      "#Iter= 14\tTrain=0.997834\tTest=0.997839\tTest(ll)=0.0217976\n",
      "#Iter= 15\tTrain=0.997833\tTest=0.997839\tTest(ll)=0.0209608\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!$LIBFM_PATH -task c -train train_fm.data -test valid_fm.data -iter 20 -dim '1,1,8' -out output.libfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hs_err_pid3747.log  lab10_solution.ipynb  lab10_true.csv\r\n",
      "Lab10.ipynb\t    Lab10_solution.ipynb  lab10_views_programmes.csv\r\n",
      "lab10_items.csv     lab10_test.csv\t  output.libfm\r\n",
      "lab10_sol.ipynb     lab10_train.csv\t  spark-FM-parallelSGD\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "LibFM training.\n",
    "\n",
    "\n",
    "anton.pilipenko@master:~/libfm-1.42.src/bin$ ./libFM -task r -train train_fm.data -test valid_fm.data -dim '1,1,5'\n",
    "#Iter= 99\tTrain=0.404857\tTest=0.410148\n",
    "\n",
    "\n",
    "anton.pilipenko@master:~/libfm-1.42.src/bin$ ./libFM -task r -train train_fm.data -test valid_fm.data -dim '1,1,8'\n",
    "#Iter= 99\tTrain=0.411104\tTest=0.409075\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "val_preds = sc.textFile('output.libfm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "l = np.loadtxt('labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "p = np.loadtxt('output.libfm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.      ,  0.41474 ],\n",
       "       [ 0.      ,  0.464436]])"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([l,p]).T[:2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
