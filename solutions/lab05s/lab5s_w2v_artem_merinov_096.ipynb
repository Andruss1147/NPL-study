{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('labeledTrainData.tsv', delimiter=\"\\t\", header=0, quoting=3)\n",
    "test = pd.read_csv('testData.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
    "unsup = pd.read_csv('unlabeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 3), (25000, 2), (50000, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape, unsup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"It takes patience to get through David Lynch\\'s eccentric, but-- for a change-- life-affirming chronicle of Alvin Straight\\'s journey, but stick with it. Though it moves as slow as Straight\\'s John Deere, when he meets the kind strangers along his pilgrimage we learn much about the isolation of aging, the painful regrets and secrets, and ultimately the power of family and reconciliation. Richard Farnsworth caps his career with the year\\'s most genuine performance, sad and poetic, flinty and caring. And Sissy Spacek matches him as his \\\\\"slow\\\\\" daughter Rose who pines over her own private loss while caring for dad. Rarely has a modern film preached so positively about family.\"',\n",
       " 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[200]['review'], train.iloc[200]['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"I may not be a critic, but here is what I think of this movie. Well just watched the movie on cinemax and first of all I just have to say how much I hate the storyline I mean come on what does a snowman scare besides little kids, secondly it is pretty gory but I bet since the movie is so low budget they probably used ketchup so MY CRITICAL VOTE IS BOMB!!! nice try and the sequel will suck twice as much.\"',\n",
       " 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[99]['review'], train.iloc[99]['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['may',\n",
       " 'not',\n",
       " 'critic',\n",
       " 'but',\n",
       " 'think',\n",
       " 'movie',\n",
       " 'well',\n",
       " 'watch',\n",
       " 'movie',\n",
       " 'cinemax',\n",
       " 'first',\n",
       " 'say',\n",
       " 'much',\n",
       " 'hate',\n",
       " 'storyline',\n",
       " 'mean',\n",
       " 'come',\n",
       " 'doe',\n",
       " 'snowman',\n",
       " 'scare',\n",
       " 'besides',\n",
       " 'little',\n",
       " 'kid',\n",
       " 'secondly',\n",
       " 'pretty',\n",
       " 'gory',\n",
       " 'but',\n",
       " 'bet',\n",
       " 'since',\n",
       " 'movie',\n",
       " 'low',\n",
       " 'budget',\n",
       " 'probably',\n",
       " 'use',\n",
       " 'ketchup',\n",
       " 'critical',\n",
       " 'vote',\n",
       " 'bomb',\n",
       " 'nice',\n",
       " 'try',\n",
       " 'sequel',\n",
       " 'suck',\n",
       " 'twice',\n",
       " 'much']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text2wordlist(text):\n",
    "    # replace negatives (don't -> do not)\n",
    "    text = re.sub('n\\'t', ' not ', text)\n",
    "    text = re.sub('nt', ' not ', text)\n",
    "    # only select words\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    words = text.lower().split()\n",
    "    # lemmatize words\n",
    "    words = [lmtzr.lemmatize(word, 'n') for word in words]\n",
    "    words = [lmtzr.lemmatize(word, 'a') for word in words]\n",
    "    words = [lmtzr.lemmatize(word, 'v') for word in words]\n",
    "    # remove stop words from text \n",
    "    # save several stop words: negative texts (0) contains negative words (not, no, etc) \n",
    "    my_stopwords = [sw for sw in set(stopwords.words('english')) if sw not in ['not', 'no', 'very', 'but',\n",
    "                                                                               'above', 'below']] \n",
    "    words = [lmtzr.lemmatize(word, 'v') for word in words if word not in my_stopwords]\n",
    "    return words\n",
    "\n",
    "text2wordlist(\"\"\"\n",
    "'\"I may not be a critic, but here is what I think of this movie. Well just watched the movie on cinemax and \n",
    "first of all I just have to say how much I hate the storyline I mean come on what does a snowman scare besides\n",
    "little kids, secondly it is pretty gory but I bet since the movie is so low budget they probably used ketchup \n",
    "so MY CRITICAL VOTE IS BOMB!!! nice try and the sequel will suck twice as much.\"'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['may', 'not', 'critic', 'but', 'think', 'movie'],\n",
       " ['well',\n",
       "  'watch',\n",
       "  'movie',\n",
       "  'cinemax',\n",
       "  'first',\n",
       "  'say',\n",
       "  'much',\n",
       "  'hate',\n",
       "  'storyline',\n",
       "  'mean',\n",
       "  'come',\n",
       "  'doe',\n",
       "  'snowman',\n",
       "  'scare',\n",
       "  'besides',\n",
       "  'little',\n",
       "  'kid',\n",
       "  'secondly',\n",
       "  'pretty',\n",
       "  'gory',\n",
       "  'but',\n",
       "  'bet',\n",
       "  'since',\n",
       "  'movie',\n",
       "  'low',\n",
       "  'budget',\n",
       "  'probably',\n",
       "  'use',\n",
       "  'ketchup',\n",
       "  'critical',\n",
       "  'vote',\n",
       "  'bomb'],\n",
       " ['nice', 'try', 'sequel', 'suck', 'twice', 'much']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text2sentences(text):\n",
    "    # remove html tags\n",
    "    text = BeautifulSoup(text, 'lxml').get_text()\n",
    "    # separate text to sentences\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(text.strip())\n",
    "    # separate sentences to words\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(text2wordlist(raw_sentence))            \n",
    "    return sentences\n",
    "\n",
    "text2sentences(\"\"\"\n",
    "'\"I may not be a critic, but here is what I think of this movie. Well just watched the movie on cinemax \n",
    "and first of all I just have to say how much I hate the storyline I mean come on what does a snowman scare\n",
    "besides little kids, secondly it is pretty gory but I bet since the movie is so low budget they probably \n",
    "used ketchup so MY CRITICAL VOTE IS BOMB!!! nice try and the sequel will suck twice as much.\"'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w2v model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [03:15<00:00, 127.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 12s, sys: 3.23 s, total: 3min 16s\n",
      "Wall time: 3min 15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sentences = []\n",
    "for review in tqdm(train['review']):\n",
    "    sentences += text2sentences(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [03:11<00:00, 130.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 9s, sys: 3.27 s, total: 3min 12s\n",
      "Wall time: 3min 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for review in tqdm(test['review']):\n",
    "    sentences += text2sentences(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [06:31<00:00, 127.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 25s, sys: 6.84 s, total: 6min 32s\n",
      "Wall time: 6min 31s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for review in tqdm(unsup['review']):\n",
    "    sentences += text2sentences(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1059231"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "dictionary = corpora.Dictionary(sentences)   # составляем словарь\n",
    "corpus = [dictionary.doc2bow(text) for text in sentences]  # составляем корпус документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 28s, sys: 1min 24s, total: 3min 52s\n",
      "Wall time: 3min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(42)\n",
    "ldamodel = models.LdaMulticore(corpus, workers=20, id2word=dictionary, num_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6,\n",
       "  [('not', 0.05376204333741923),\n",
       "   ('character', 0.014844934679216357),\n",
       "   ('but', 0.009855161645466343),\n",
       "   ('doe', 0.007530977342214983),\n",
       "   ('one', 0.007269178027699474),\n",
       "   ('like', 0.006551164832035609),\n",
       "   ('actor', 0.006389247975043039),\n",
       "   ('movie', 0.006326466319359115),\n",
       "   ('film', 0.0062676844877654035),\n",
       "   ('performance', 0.005405867063733255)]),\n",
       " (17,\n",
       "  [('not', 0.03954013393021745),\n",
       "   ('but', 0.03158350597648299),\n",
       "   ('see', 0.031090422613056952),\n",
       "   ('movie', 0.026006981974337436),\n",
       "   ('wa', 0.018000050070880864),\n",
       "   ('get', 0.013941662123378844),\n",
       "   ('film', 0.010350212400966714),\n",
       "   ('time', 0.008906628657249182),\n",
       "   ('good', 0.008390950177483113),\n",
       "   ('bad', 0.007210367678166351)])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_ing_n = ldamodel.show_topics(num_topics=2, num_words=10, formatted=False)\n",
    "top_ing_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# .......................................... not rly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50min 11s, sys: 8.92 s, total: 50min 20s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "num_features = 300 # размерность вектора каждого слова\n",
    "min_word_count = 5 # минимальная частотность слова, чтобы она попала в модель\n",
    "num_workers = 20 # кол-во ядер процессора\n",
    "context = 10 # размер окна\n",
    "downsampling = 2e-3 # probability\n",
    "skip_gram = 1 # 0 or 1\n",
    "# negative_sampling = 10 # If > 0: how many “noise words” should be drawn\n",
    "# epochs = 3 # Number of iterations (epochs) over the corpus.\n",
    "\n",
    "model = Word2Vec(sentences, \n",
    "                 workers=num_workers, \n",
    "                 size=num_features, \n",
    "                 min_count=min_word_count, \n",
    "                 window=context, \n",
    "                 sample=downsampling, \n",
    "                 sg=skip_gram, \n",
    "#                  negative=negative_sampling,\n",
    "#                  iter=epochs\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.init_sims(replace=True) # финализируем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('critically', 0.5451637506484985),\n",
       " ('flak', 0.5068663954734802),\n",
       " ('laudatory', 0.4998537600040436),\n",
       " ('undeserved', 0.4995395243167877),\n",
       " ('lenie', 0.4949934780597687),\n",
       " ('critic', 0.4895322918891907),\n",
       " ('reappraisal', 0.48356980085372925),\n",
       " ('rebuttal', 0.47935742139816284),\n",
       " ('unreasonably', 0.4787167012691498),\n",
       " ('unmercifully', 0.4784488081932068)]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('critical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marmite', 0.569459080696106),\n",
       " ('despise', 0.5493627190589905),\n",
       " ('hater', 0.530159056186676),\n",
       " ('otakus', 0.5273692011833191),\n",
       " ('dislike', 0.5256993770599365),\n",
       " ('catdog', 0.515252947807312),\n",
       " ('sterotype', 0.5144835710525513),\n",
       " ('detest', 0.508788526058197),\n",
       " ('love', 0.5044335126876831),\n",
       " ('charactors', 0.5035682916641235)]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('hate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wonderful', 0.668364405632019),\n",
       " ('terrific', 0.6540890336036682),\n",
       " ('good', 0.618759036064148),\n",
       " ('awsome', 0.6133989691734314),\n",
       " ('excelle', 0.6120513677597046),\n",
       " ('outstanding', 0.6020387411117554),\n",
       " ('fine', 0.6007612943649292),\n",
       " ('excele', 0.5831233263015747),\n",
       " ('superb', 0.5823565721511841),\n",
       " ('incredible', 0.5810086727142334)]"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text2vec(words, model, size):\n",
    "    text_vec = np.zeros((size,), dtype='float32')\n",
    "    n_words = 0    \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            text_vec = np.add(text_vec, model[word])    \n",
    "    if n_words != 0:\n",
    "        text_vec /= n_words    \n",
    "    return text_vec    \n",
    "\n",
    "def texts2vecs(texts, model, size):\n",
    "    texts_vecs = np.zeros((len(texts), size), dtype='float32')    \n",
    "    for i, text in enumerate(texts):\n",
    "        texts_vecs[i] = text2vec(text, model, size)        \n",
    "    return texts_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 53s, sys: 6.91 s, total: 5min\n",
      "Wall time: 4min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_like_word_list = [sum(text2sentences(text), []) for text in train['review']]\n",
    "train_vecs = texts2vecs(train_like_word_list, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 300)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 47s, sys: 2.65 s, total: 4min 50s\n",
      "Wall time: 4min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_like_word_list = [sum(text2sentences(text), []) for text in test['review']]\n",
    "test_vecs = texts2vecs(test_like_word_list, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 300)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# forest = RandomForestClassifier(n_estimators=1000, n_jobs=20)\n",
    "# forest = forest.fit(train_vecs, train['sentiment'])\n",
    "\n",
    "# predict = forest.predict(test_vecs)\n",
    "\n",
    "# # Copy the results to a pandas dataframe with an \"id\" column an a \"sentiment\" column\n",
    "# output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":predict})\n",
    "\n",
    "# # Use pandas to write the comma-separated output file\n",
    "# output.to_csv(\"w2v_rf_1000_results.csv\", index=False, quoting=3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'max_depth': 10, \n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.1,   \n",
    "    'silent': 1,\n",
    "    'nthread': 15,\n",
    "    'subsample': 0.95, \n",
    "    'colsample_bytree': 0.95,\n",
    "    'colsample_bylevel': 1.0,\n",
    "    'min_child_weight': 2.0, \n",
    "    'scale_pos_weight': 1.0,\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metrics':'accuracy', \n",
    "    'eta': 0.275,\n",
    "    'alpha': 0.05,\n",
    "    'gamma': 0.61,\n",
    "    'seed': 27\n",
    "    }\n",
    "\n",
    "dtrain = xgb.DMatrix(train_vecs, label=train['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.11428\n",
      "[1]\ttrain-error:0.0846\n",
      "[2]\ttrain-error:0.06644\n",
      "[3]\ttrain-error:0.05972\n",
      "[4]\ttrain-error:0.0536\n",
      "[5]\ttrain-error:0.05136\n",
      "[6]\ttrain-error:0.0456\n",
      "[7]\ttrain-error:0.04352\n",
      "[8]\ttrain-error:0.04032\n",
      "[9]\ttrain-error:0.03872\n",
      "[10]\ttrain-error:0.0356\n",
      "[11]\ttrain-error:0.0322\n",
      "[12]\ttrain-error:0.0306\n",
      "[13]\ttrain-error:0.02812\n",
      "[14]\ttrain-error:0.02632\n",
      "[15]\ttrain-error:0.02408\n",
      "[16]\ttrain-error:0.02144\n",
      "[17]\ttrain-error:0.01976\n",
      "[18]\ttrain-error:0.0184\n",
      "[19]\ttrain-error:0.01712\n",
      "[20]\ttrain-error:0.01632\n",
      "[21]\ttrain-error:0.01504\n",
      "[22]\ttrain-error:0.01444\n",
      "[23]\ttrain-error:0.01348\n",
      "[24]\ttrain-error:0.01236\n",
      "[25]\ttrain-error:0.01152\n",
      "[26]\ttrain-error:0.01064\n",
      "[27]\ttrain-error:0.00988\n",
      "[28]\ttrain-error:0.0092\n",
      "[29]\ttrain-error:0.00836\n",
      "[30]\ttrain-error:0.00748\n",
      "[31]\ttrain-error:0.00696\n",
      "[32]\ttrain-error:0.0062\n",
      "[33]\ttrain-error:0.00556\n",
      "[34]\ttrain-error:0.00524\n",
      "[35]\ttrain-error:0.0048\n",
      "[36]\ttrain-error:0.0044\n",
      "[37]\ttrain-error:0.00376\n",
      "[38]\ttrain-error:0.00368\n",
      "[39]\ttrain-error:0.00356\n",
      "[40]\ttrain-error:0.00308\n",
      "[41]\ttrain-error:0.00316\n",
      "[42]\ttrain-error:0.00292\n",
      "[43]\ttrain-error:0.0028\n",
      "[44]\ttrain-error:0.00236\n",
      "[45]\ttrain-error:0.00232\n",
      "[46]\ttrain-error:0.00192\n",
      "[47]\ttrain-error:0.00172\n",
      "[48]\ttrain-error:0.00152\n",
      "[49]\ttrain-error:0.00132\n",
      "[50]\ttrain-error:0.00128\n",
      "[51]\ttrain-error:0.00116\n",
      "[52]\ttrain-error:0.00112\n",
      "[53]\ttrain-error:0.00104\n",
      "[54]\ttrain-error:0.00084\n",
      "[55]\ttrain-error:0.00056\n",
      "[56]\ttrain-error:0.00044\n",
      "[57]\ttrain-error:0.0004\n",
      "[58]\ttrain-error:0.0004\n",
      "[59]\ttrain-error:0.0004\n",
      "[60]\ttrain-error:0.0004\n",
      "[61]\ttrain-error:0.0004\n",
      "[62]\ttrain-error:0.0004\n",
      "[63]\ttrain-error:0.00032\n",
      "[64]\ttrain-error:0.00028\n",
      "[65]\ttrain-error:0.0002\n",
      "[66]\ttrain-error:0.00016\n",
      "[67]\ttrain-error:0.00012\n",
      "[68]\ttrain-error:0.00012\n",
      "[69]\ttrain-error:8e-05\n",
      "[70]\ttrain-error:8e-05\n",
      "[71]\ttrain-error:8e-05\n",
      "[72]\ttrain-error:4e-05\n",
      "[73]\ttrain-error:4e-05\n",
      "[74]\ttrain-error:4e-05\n",
      "[75]\ttrain-error:4e-05\n",
      "[76]\ttrain-error:4e-05\n",
      "[77]\ttrain-error:4e-05\n",
      "[78]\ttrain-error:4e-05\n",
      "[79]\ttrain-error:4e-05\n",
      "[80]\ttrain-error:0\n",
      "[81]\ttrain-error:0\n",
      "[82]\ttrain-error:0\n",
      "[83]\ttrain-error:0\n",
      "[84]\ttrain-error:0\n",
      "[85]\ttrain-error:0\n",
      "[86]\ttrain-error:0\n",
      "[87]\ttrain-error:0\n",
      "[88]\ttrain-error:0\n",
      "[89]\ttrain-error:0\n",
      "[90]\ttrain-error:0\n",
      "[91]\ttrain-error:0\n",
      "[92]\ttrain-error:0\n",
      "[93]\ttrain-error:0\n",
      "[94]\ttrain-error:0\n",
      "[95]\ttrain-error:0\n",
      "[96]\ttrain-error:0\n",
      "[97]\ttrain-error:0\n",
      "[98]\ttrain-error:0\n",
      "[99]\ttrain-error:0\n",
      "[100]\ttrain-error:0\n",
      "[101]\ttrain-error:0\n",
      "[102]\ttrain-error:0\n",
      "[103]\ttrain-error:0\n",
      "[104]\ttrain-error:0\n",
      "[105]\ttrain-error:0\n",
      "[106]\ttrain-error:0\n",
      "[107]\ttrain-error:0\n",
      "[108]\ttrain-error:0\n",
      "[109]\ttrain-error:0\n",
      "[110]\ttrain-error:0\n",
      "[111]\ttrain-error:0\n",
      "[112]\ttrain-error:0\n",
      "[113]\ttrain-error:0\n",
      "[114]\ttrain-error:0\n",
      "[115]\ttrain-error:0\n",
      "[116]\ttrain-error:0\n",
      "[117]\ttrain-error:0\n",
      "[118]\ttrain-error:0\n",
      "[119]\ttrain-error:0\n",
      "[120]\ttrain-error:0\n",
      "[121]\ttrain-error:0\n",
      "[122]\ttrain-error:0\n",
      "[123]\ttrain-error:0\n",
      "[124]\ttrain-error:0\n",
      "[125]\ttrain-error:0\n",
      "[126]\ttrain-error:0\n",
      "[127]\ttrain-error:0\n",
      "[128]\ttrain-error:0\n",
      "[129]\ttrain-error:0\n",
      "[130]\ttrain-error:0\n",
      "[131]\ttrain-error:0\n",
      "[132]\ttrain-error:0\n",
      "[133]\ttrain-error:0\n",
      "[134]\ttrain-error:0\n",
      "[135]\ttrain-error:0\n",
      "[136]\ttrain-error:0\n",
      "[137]\ttrain-error:0\n",
      "[138]\ttrain-error:0\n",
      "[139]\ttrain-error:0\n",
      "[140]\ttrain-error:0\n",
      "[141]\ttrain-error:0\n",
      "[142]\ttrain-error:0\n",
      "[143]\ttrain-error:0\n",
      "[144]\ttrain-error:0\n",
      "[145]\ttrain-error:0\n",
      "[146]\ttrain-error:0\n",
      "[147]\ttrain-error:0\n",
      "[148]\ttrain-error:0\n",
      "[149]\ttrain-error:0\n"
     ]
    }
   ],
   "source": [
    "watchlist = [(dtrain, 'train')]\n",
    "# num_boost_round: number of boosting iterations \n",
    "bst = xgb.train(xgb_params, dtrain, num_boost_round=150, evals=watchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(test_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ypred = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.9884903e-01, 9.0808643e-04, 3.5563290e-01, ..., 6.6848427e-01,\n",
       "       9.9912506e-01, 1.6750295e-01], dtype=float32)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the results to a pandas dataframe with an \"id\" column an a \"sentiment\" column\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":ypred})\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv(\"w2v_xgb_results_5.csv\", index=False, quoting=3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
